{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder,OneHotEncoder #this is optional\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,r2_score,roc_auc_score,classification_report,mean_squared_error,accuracy_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:/datasets+minipro/Vidhya k SLC/Bank_Personal_Loan_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('ID',axis=1,inplace=True)\n",
    "\n",
    "df.drop('ZIP Code',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating X & y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Personal Loan',axis=1)\n",
    "y=df[['Personal Loan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting using train test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting X&y using train_test:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.172164\n",
      "1  0.164152\n",
      "2  0.086702\n",
      "3  0.014368\n",
      "4  0.048675\n",
      "roc_auc_score on training data:  0.8925367526184774\n",
      "roc_auc_score on testing data:  0.8970209478021977\n"
     ]
    }
   ],
   "source": [
    "#logreg = LogisticRegression(penalty=\"l1\",class_weight=\"balanced\",random_state=8) # logistic regression using lasso\n",
    "logreg = LogisticRegression(penalty=\"l2\",class_weight=\"balanced\",random_state=8) # logistic regression using Ridge,by default ridge is used\n",
    "logreg.fit(X_train,y_train)\n",
    "y_tr_pred=logreg.predict(X_train)\n",
    "y_test_pred=logreg.predict(X_test)\n",
    "y_prob = logreg.predict_proba(X_test)\n",
    "print(pd.DataFrame(y_prob[:,1]).head())  #### head of probability predicted\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('roc_auc_score on training data: ',roc_auc_score(y_train,y_tr_pred)) #train\n",
    "print('roc_auc_score on testing data: ',roc_auc_score(y_test,y_test_pred)) #test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Logistic regression using metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[789 107]\n",
      " [  9  95]]\n",
      "----------------------------------------------\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93       896\n",
      "           1       0.47      0.91      0.62       104\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.73      0.90      0.78      1000\n",
      "weighted avg       0.93      0.88      0.90      1000\n",
      "\n",
      "----------------------------------------------\n",
      "log loss:\n",
      "0.28202695584985393\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix:\n",
    "cm0 = confusion_matrix(y_test, y_test_pred)\n",
    "print('Confusion matrix:')\n",
    "print(cm0)\n",
    "print('----------------------------------------------')\n",
    "\n",
    "#Classification report of precision,accuracy,recall:\n",
    "#optional #cr=classification_report(y_test, y_pred)\n",
    "#optional#print(cr)\n",
    "print('classification report:')\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print('----------------------------------------------')\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('log loss:')\n",
    "print(log_loss(y_test,y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the decision tree with default hyperparameters, apart from\n",
    "# max_depth which is 5 so that we can plot and read the tree.\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Decision Tree using metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1=confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       896\n",
      "           1       0.97      0.84      0.90       104\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.97      0.92      0.94      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "----------------------------------------------\n",
      "Confusion matrix:\n",
      "[[893   3]\n",
      " [ 17  87]]\n",
      "----------------------------------------------\n",
      "accuracy score:\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# dt.score(y_test,y_pred)\n",
    "\n",
    "print('classification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('----------------------------------------------')\n",
    "# Printing confusion matrix and accuracy\n",
    "print('Confusion matrix:')\n",
    "print(cm1)\n",
    "print('----------------------------------------------')\n",
    "print('accuracy score:')\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "-requires large memory for storing the entire training dataset for prediction. \n",
    " KNN requires ***scaling of data[imp]***** because KNN uses the Euclidean distance between two\n",
    " data points to find nearest neighbors.\n",
    "-Euclidean distance is sensitive to magnitudes. \n",
    "-The features with high magnitudes will weight more than features with low magnitudes.\n",
    "-KNN also not suitable for large dimensional data.\n",
    "'''\n",
    "\n",
    "'''\n",
    "-The training phase of K-nearest neighbor classification is much faster compared to \n",
    " other classification algorithms. There is no need to train a model for generalization,\n",
    "-That is why KNN is known as the simple and instance-based learning algorithm.\n",
    "-KNN can be useful in case of nonlinear data. It can be used with the regression problem. \n",
    " Output value for the object is computed by the average of k closest neighbors value.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of neighbours 1 is:  0.902\n",
      "num of neighbours 2 is:  0.913\n",
      "num of neighbours 3 is:  0.907\n",
      "num of neighbours 4 is:  0.909\n",
      "num of neighbours 5 is:  0.903\n",
      "num of neighbours 6 is:  0.905\n",
      "num of neighbours 7 is:  0.905\n",
      "num of neighbours 8 is:  0.907\n",
      "num of neighbours 9 is:  0.905\n",
      "num of neighbours 10 is:  0.901\n",
      "Max score by changing value of k:  0.913\n",
      "minimum value of mean square error:  0.08699999999999997\n",
      "optimum value for k is at index: [1]\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(1,11,1):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('num of neighbours',i,'is: ',accuracy_score(y_test, y_pred))\n",
    "    l.append(accuracy_score(y_test, y_pred))\n",
    "print('Max score by changing value of k: ',max(l))\n",
    "\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in l]\n",
    "print('minimum value of mean square error: ',min(MSE))\n",
    "optimal_k = [MSE.index(min(MSE))]\n",
    "print('optimum value for k is at index:',optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of neighbours 2 accuracy is:  0.913\n"
     ]
    }
   ],
   "source": [
    "    knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('num of neighbours' ,'2 accuracy ''is: ',accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc= RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Income</td>\n",
       "      <td>0.324735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.193552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCAvg</td>\n",
       "      <td>0.176080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Family</td>\n",
       "      <td>0.111726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CD Account</td>\n",
       "      <td>0.056558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experience</td>\n",
       "      <td>0.040973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.038709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>0.036158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CreditCard</td>\n",
       "      <td>0.008542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Online</td>\n",
       "      <td>0.007959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Securities Account</td>\n",
       "      <td>0.005008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "2               Income    0.324735\n",
       "5            Education    0.193552\n",
       "4                CCAvg    0.176080\n",
       "3               Family    0.111726\n",
       "8           CD Account    0.056558\n",
       "1           Experience    0.040973\n",
       "0                  Age    0.038709\n",
       "6             Mortgage    0.036158\n",
       "10          CreditCard    0.008542\n",
       "9               Online    0.007959\n",
       "7   Securities Account    0.005008"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Important features\n",
    "feat_imp_df=pd.DataFrame({'features':X_train.columns,'importance':rfc.feature_importances_})\n",
    "feat_imp_df.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880000000000001\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "n_estimators=range(100,700,100)\n",
    "criterion=['gini','entropy']\n",
    "hyper={'n_estimators':n_estimators,\n",
    "      'criterion':criterion}\n",
    "\n",
    "### Grid search CV for Random forest classifier:\n",
    "\n",
    "gd=GridSearchCV(estimator=RandomForestClassifier(random_state=8),param_grid=hyper,verbose=True)\n",
    "gd.fit(X_train,y_train)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Bagging classifier with knn and using K fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged KNN is: 0.906\n",
      "The cross validated score for bagged KNN is: 0.9128000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bgcl_knn=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=8,\n",
    "                           n_estimators=200)\n",
    "bgcl_knn.fit(X_train,y_train)\n",
    "prediction0=bgcl_knn.predict(X_test)\n",
    "print('The accuracy for bagged KNN is:',accuracy_score(y_test,prediction0))\n",
    "result=cross_val_score(bgcl_knn,X,y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged KNN is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Bagging classifier with Decision Tree and using K fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged Decision Tree is: 0.989\n",
      "The cross validated score for bagged Decision Tree is: 0.9870000000000001\n"
     ]
    }
   ],
   "source": [
    "## Even if we dont mention anything to base_estimator,default is decisiontreeclassifier\n",
    "\n",
    "bgcl_dt=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=8\n",
    "                        ,n_estimators=100)\n",
    "bgcl_dt.fit(X_train,y_train)\n",
    "prediction1=bgcl_dt.predict(X_test)\n",
    "print('The accuracy for bagged Decision Tree is:',accuracy_score(prediction1,y_test))\n",
    "result=cross_val_score(bgcl_dt,X,y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged Decision Tree is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Bagging classifier with Random Forest and using K fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged Decision Tree is: 0.986\n"
     ]
    }
   ],
   "source": [
    "#### this code takes a lot of time to run computationally #### \n",
    "bgcl_rfc=BaggingClassifier(base_estimator=RandomForestClassifier(),random_state=8 ,n_estimators=50)\n",
    "bgcl_rfc.fit(X_train,y_train)\n",
    "prediction2=bgcl_rfc.predict(X_test)\n",
    "print('The accuracy for bagged Decision Tree is:',accuracy_score(prediction2,y_test))\n",
    "\n",
    "### Warning!!!!!!!!!!!!!!!!!!!!! ########\n",
    "#the below mentioned 2 lines of code takes a lot of time to run,so avoid running k fold/gridsearch CV for rfc\n",
    "# result=cross_val_score(bgcl_rfc,X,y,cv=10,scoring='accuracy')\n",
    "# print('The cross validated score for bagged Decision Tree is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Bagging classifier with Naivebayes[gaussian] and using K fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Naive bayes is: 0.873\n"
     ]
    }
   ],
   "source": [
    "bgcl_nb=BaggingClassifier(base_estimator=GaussianNB(),random_state=8,n_estimators=51)\n",
    "bgcl_nb.fit(X_train,y_train)\n",
    "prediction3=bgcl_nb.predict(X_test)\n",
    "print('The accuracy for Naive bayes is:',accuracy_score(prediction3,y_test))\n",
    "# print('The accuracy for bagged Decision Tree is:',accuracy_score(prediction1,y_test))\n",
    "# result=cross_val_score(bgcl_nb,X,y,cv=10,scoring='accuracy')\n",
    "# print('The cross validated score for bagged Decision Tree is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-Bagging classifier with Logistic Regression and using K fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged Logistic Regression is: 0.986\n"
     ]
    }
   ],
   "source": [
    "bgcl_LR=BaggingClassifier(base_estimator=LogisticRegression(),random_state=8,n_estimators=50)\n",
    "bgcl_LR.fit(X_train,y_train)\n",
    "prediction4=bgcl_LR.predict(X_test)\n",
    "print('The accuracy for bagged Logistic Regression is:',accuracy_score(prediction4,y_test))\n",
    "# print('The accuracy for bagged Decision Tree is:',accuracy_score(prediction1,y_test))\n",
    "# result=cross_val_score(bgcl_LR,X,y,cv=10,scoring='accuracy')\n",
    "# print('The cross validated score for bagged Logistic Regression is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Adaptive boosting/Adaboost :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score wrto xtest,ytest: 0.968\n",
      "score wrto ytest,ypred: 0.968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(random_state=1)\n",
    "abc.fit(X_train, y_train)\n",
    "print('score wrto xtest,ytest:',abc.score(X_test,y_test))\n",
    "y_pred=abc.predict(X_test)\n",
    "print('score wrto ytest,ypred:',accuracy_score(y_test,y_pred)) ## actual measure of how accurate is model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of all validations:  [0.968 0.978 0.964 0.97  0.966 0.972 0.968 0.974 0.962 0.968]\n",
      "The cross validated score for AdaBoost is: 0.969\n"
     ]
    }
   ],
   "source": [
    "###### cross validation of ada boost classifier using ***k fold*** ########\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\n",
    "result=cross_val_score(ada,X,y,cv=10,scoring='accuracy')\n",
    "print('result of all validations: ',result)\n",
    "print('The cross validated score for AdaBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_estimators=list(range(100,500,100))\\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\\ngd.fit(X_train,y_train)\\nprint(gd.best_score_)\\nprint(gd.best_estimator_)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Warning ########\n",
    "#the below mentioned lines of code takes a lot of time to run!!!!!!!!!!!!!!!!!!!!!1\n",
    "\n",
    "###### cross validation of ada boost classifier using grid search CV ########[Use this at ur own risk!]\n",
    "'''\n",
    "n_estimators=list(range(100,500,100))\n",
    "learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\n",
    "gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X_train,y_train)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Gradient Boosting : We cant apply any other model than decision tree [it uses dt by default]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Warning :    We only use dt by default,no base_estimator argument in gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc= GradientBoostingClassifier(learning_rate=0.01,random_state=8)\n",
    "#learning rate should be between 0 to 1,good sp is 0.01 or 0.1\n",
    "gbc.fit(X_train, y_train)\n",
    "gbc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of all validations:  [0.988 0.984 0.988 0.986 0.984 0.982 0.988 0.99  0.988 0.984]\n",
      "The cross validated score for Gradient Boosting is: 0.9862\n"
     ]
    }
   ],
   "source": [
    "###### cross validation of ada boost classifier using ***k fold*** ########\n",
    "grad=GradientBoostingClassifier(n_estimators=100,random_state=8,learning_rate=0.1)\n",
    "result=cross_val_score(grad,X,y,cv=10,scoring='accuracy')\n",
    "print('result of all validations: ',result)\n",
    "print('The cross validated score for Gradient Boosting is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_estimators=list(range(100,500,100))\\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\\ngd=GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=hyper,verbose=True)\\ngd.fit(X_train,y_train)\\nprint(gd.best_score_)\\nprint(gd.best_estimator_)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Warning ########\n",
    "#the below mentioned lines of code takes a lot of time to run!!!!!!!!!!!!!!!!!!!!!1\n",
    "\n",
    "###### cross validation of ada boost classifier using grid search CV ########[Use this at ur own risk!]\n",
    "'''\n",
    "n_estimators=list(range(100,500,100))\n",
    "learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\n",
    "gd=GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X_train,y_train)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=df.drop('Personal Loan',axis=1)\n",
    "# y=df[['Personal Loan']]\n",
    "\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# X = sm.add_constant(X)\n",
    "# model = sm.OLS(y, X).fit()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Hierarchial Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
